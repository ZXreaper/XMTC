{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_process_only_content_information\n",
    "- embedding matrix    shape:(所有词的个数，词向量大小) ---------------------------------------------------------> embedding_matrix.npy\n",
    "- vocab               词集 shape:(词的个数) ------------------------------------------------------------------------------------->  vocab.npy\n",
    "- labels binarizer    经过二进制转换过的labels\n",
    "- train_text_tokenizer 训练集的 tokenizer shape:(训练集数目，每条数据的维度) --------------------------------> train_texts_tokenizer.npy\n",
    "- train_text_labels   训练集的labels  shape:(训练集数目，) 每条数据的标签数目是不同的，是个二维数组 -------> train_labels.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = pd.read_csv('./data/only_textinformation_feed_content_label.csv')\n",
    "org_data = org_data.drop(columns=['Unnamed: 0'])\n",
    "# 将content为空的数据去除\n",
    "org_data = org_data[org_data['content'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有动态的文本内容\n",
    "df = org_data[['content', 'label_ids']]\n",
    "\n",
    "# 读取停用词表\n",
    "stop_word_file = './data/stop_words.npy'\n",
    "stop_words = np.load(stop_word_file).tolist()\n",
    "\n",
    "# 读取领域词汇\n",
    "jieba.load_userdict('./data/new_dict.txt')\n",
    "\n",
    "# 读入量词表\n",
    "quantifier_dict = list()\n",
    "with open('./data/quantifier_dict.txt', 'r') as f:\n",
    "    my_data = f.readlines()\n",
    "    for line in my_data:\n",
    "        quantifier_dict.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除数字和其后面的量词\n",
    "# 只有数字后面跟的是量词的时候才会把量词和数字都去掉，否则不去掉数字\n",
    "def remove_quantifier(s : str) -> str:\n",
    "    digit = re.findall(r'\\d+', s)\n",
    "    order = [i.start() for i in re.finditer(r'\\d+', s)] \n",
    "#     for index, number in enumerate(digit):\n",
    "#         print(index, number,'\\n')\n",
    "    res = ''\n",
    "    last_pos = 0\n",
    "    arr_size = len(digit)\n",
    "    for i in range(arr_size):\n",
    "        number, pos = digit[i], order[i]\n",
    "        next_pos = pos+len(number)\n",
    "        res = res + s[last_pos : pos]\n",
    "        if next_pos < len(s):\n",
    "            next_word = s[next_pos]\n",
    "            if next_word in quantifier_dict:\n",
    "                # 对数字+量词进行删除\n",
    "                last_pos = next_pos + 1\n",
    "            else:\n",
    "                last_pos = pos\n",
    "    res = res + s[last_pos : ]\n",
    "    return res\n",
    "\n",
    "    # 分词\n",
    "# 分词 -> 保证出来的是list\n",
    "def content_cut(x):\n",
    "    x = remove_quantifier(x) # 移除量词\n",
    "    x = re.sub(emoji.get_emoji_regexp(), '', x)   # 删除表情\n",
    "    x = re.sub('(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]', '', x)   # 删除URL\n",
    "    x = re.sub('\\[.*?\\]', '', x)   # 删除所有[...]\n",
    "    x = re.sub('<br>', '。', x)   # 将<br>替换为‘,’\n",
    "    x = re.sub('<.*?>', '', x)   # 删除所有<...>\n",
    "    x = re.sub('&nbsp', ' ', x)   # 将&nbsp替换为空格\n",
    "    x = re.sub('\\\\u200b', '', x)   # 删除所有\\u200b\n",
    "#     x = re.sub('[^A-Za-z\\u4e00-\\u9fa5]+', ' ', x)   # 将中文字符和字母以外的字符换成空格\n",
    "    x = re.sub('[^\\u4e00-\\u9fa5]+',' ',x)   # 将中文字符以外的字符换成空格\n",
    "    \n",
    "    x = jieba.lcut(x, cut_all=False, HMM=True)\n",
    "    x = \" \".join(x)\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词：处理中文、处理停用词\n",
    "id_org_content = dict()       # id号 索引 原文\n",
    "id_cuted_content = dict()     # id号 索引 分词后的结果\n",
    "content_cuted = list()        # 只有分词后的结果\n",
    "labels = list()               # 标签. [['1','334'],['1'],...]\n",
    "for index, val in df.iterrows():\n",
    "    v, l = val[0], str2list(val[1])\n",
    "    temp = list()\n",
    "    \n",
    "    if isinstance(v, float):\n",
    "        # content内容为'空'的情况\n",
    "        # temp.append('空')\n",
    "        continue\n",
    "    v = content_cut(v)\n",
    "    for i in v:\n",
    "        if i not in stop_words and i != '丨':\n",
    "            temp.append(i)\n",
    "    if len(temp) == 0:\n",
    "        # 经过去停用词后，content变为空的情况去掉\n",
    "        continue\n",
    "    \n",
    "    id_org_content[index] = v\n",
    "    id_cuted_content[index] = temp\n",
    "    content_cuted.append(temp)\n",
    "    labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id_org_content))\n",
    "print(len(id_cuted_content))\n",
    "print(len(content_cuted))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/id_org_content.npy', id_org_content)\n",
    "np.save('./data/id_cuted_content.npy', id_cuted_content)\n",
    "np.save('./data/content_cuted.npy', content_cuted)\n",
    "np.save('./data/train_labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [len(v) for v in content_cuted]\n",
    "\n",
    "# 长度 30 可以覆盖超过91%的动态\n",
    "np.percentile(x, 91)\n",
    "\n",
    "MAX_LEN = 30\n",
    "\n",
    "# 将出现频率小的单词去除掉\n",
    "# word_dict的结构是 {'单词': [单词新的标号, 出现次数]}\n",
    "word_dict_raw = {'PADDING': [0, 999999]}\n",
    "# word_dict_raw = dict()\n",
    "\n",
    "for feed in content_cuted:\n",
    "    for word in feed:\n",
    "        if word in word_dict_raw:\n",
    "            word_dict_raw[word][1] += 1\n",
    "        else:\n",
    "            word_dict_raw[word] = [len(word_dict_raw), 1]\n",
    "\n",
    "# # 查看去掉的是什么词\n",
    "# for k,v in word_dict_raw.items():\n",
    "#     if v[1] <= 1:\n",
    "#         print(k)\n",
    "\n",
    "# # 将出现频率小的单词去除掉\n",
    "# # word_dict的结构是 {'单词': [单词新的标号, 出现次数]}\n",
    "# word_dict = dict()\n",
    "\n",
    "# for i in word_dict_raw:\n",
    "#     if word_dict_raw[i][1] >= 2:\n",
    "#         word_dict[i] = [len(word_dict), word_dict_raw[i][1]]\n",
    "\n",
    "word_dict = word_dict_raw\n",
    "        \n",
    "# print(len(word_dict), len(word_dict_raw))\n",
    "\n",
    "id_cuted_tokenizer = dict()\n",
    "feed_words = []\n",
    "# feed_words = [[0] * MAX_LEN]\n",
    "for index, feed in id_cuted_content.items():\n",
    "    word_id = []\n",
    "    for word in feed:\n",
    "        if word in word_dict:\n",
    "            word_id.append(word_dict[word][0])\n",
    "    word_id = word_id[:MAX_LEN]                                    # 截取。一个标题最多30个单词。\n",
    "    feed_words.append(word_id + [0] * (MAX_LEN - len(word_id)))    # 填充。不足30个单词的标题，补0。\n",
    "    id_cuted_tokenizer[index] = np.array(word_id + [0] * (MAX_LEN - len(word_id)), dtype='int32')\n",
    "feed_words = np.array(feed_words, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/train_texts_tokenizer.npy', feed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(feed_words.shape[0]):\n",
    "    if (feed_words[i] != 0).sum() == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word_dict.keys())\n",
    "np.save('./data/vocab.npy', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取中文的embedding vector模型\n",
    "# 读取预训练好的中文的embedding vector模型\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# sina word\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('../feed_text_classification/代码规整/utils/sgns.weibo.word', binary=False)\n",
    "\n",
    "embedding_dict = {}\n",
    "\n",
    "for k, v in word_dict.items():\n",
    "    if k in word2vec_model:\n",
    "        embedding_dict[k] = word2vec_model[k]\n",
    "        \n",
    "embedding_matrix = [0]*len(word_dict)    # embedding_matrix的用法是：输入单词的编号，得到单词的索引\n",
    "\n",
    "for i in embedding_dict:\n",
    "    embedding_matrix[word_dict[i][0]] = np.array(embedding_dict[i], dtype='float32')\n",
    "\n",
    "\n",
    "words_list = list(word_dict.keys())\n",
    "for i in range(len(embedding_matrix)):\n",
    "    if type(embedding_matrix[i]) == int:                      # 袋外词处理:利用单词中每个字的平均处理\n",
    "        embedding_matrix[i] = np.zeros(300, dtype='float32')\n",
    "        word = words_list[i]\n",
    "        for c in word:\n",
    "            if c in word2vec_model:\n",
    "                embedding_matrix[i] += word2vec_model[c]\n",
    "        embedding_matrix[i] = embedding_matrix[i] / len(word)\n",
    "        \n",
    "embedding_matrix[0] = np.zeros(300, dtype='float32')\n",
    "embedding_matrix = np.array(embedding_matrix, dtype='float32')\n",
    "\n",
    "# 8662个词\n",
    "np.save('./data/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9656ca5d94ad0850ce6dda04bf402d309418121c396964e78e1f3502b86ac7e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('py38-tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
